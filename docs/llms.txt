# MAIA - Memory AI Architecture
# LLM-Friendly Documentation
# Version: 1.0.0
# Last Updated: 2026-01-20

================================================================================
OVERVIEW
================================================================================

MAIA (Memory AI Architecture) is an AI-native distributed memory system for LLM context management. It provides intelligent context assembly that sits between applications and LLMs, addressing the "context rot" problem where LLM accuracy varies 20-30% based on information position.

CORE CAPABILITIES:
- Position-aware context assembly (Critical → Middle → Recency zones)
- Multi-strategy retrieval (vector, full-text, recency, frequency, graph)
- Multi-tenancy with quota management
- MCP (Model Context Protocol) integration for Claude Desktop and Cursor
- OpenAI-compatible proxy with automatic memory injection/extraction
- Pluggable inference system with multiple backend providers
- Local embeddings via ONNX runtime (no external API calls needed)

TECH STACK:
- Language: Go 1.22+
- Storage: BadgerDB (embedded LSM-tree)
- Vector Index: HNSW (Hierarchical Navigable Small World)
- Full-Text Index: Bleve v2
- Embedding: Local ONNX (all-MiniLM-L6-v2, 384 dimensions) or remote providers
- HTTP Framework: Gin
- MCP: Model Context Protocol SDK

================================================================================
QUICK START
================================================================================

INSTALLATION:

# Docker (recommended)
docker run -d -p 8080:8080 -v maia-data:/data ghcr.io/ar4mirez/maia:latest

# From source
git clone https://github.com/ar4mirez/maia
cd maia
make build
./build/maia

# Go install
go install github.com/ar4mirez/maia/cmd/maia@latest
go install github.com/ar4mirez/maia/cmd/maiactl@latest

VERIFY:
curl http://localhost:8080/health
# Response: {"status":"ok"}

BASIC OPERATIONS:

# Store a memory
curl -X POST http://localhost:8080/v1/memories \
  -H "Content-Type: application/json" \
  -d '{
    "namespace": "default",
    "content": "User prefers dark mode and compact layouts",
    "type": "semantic",
    "tags": ["preferences", "ui"]
  }'

# Search memories
curl -X POST http://localhost:8080/v1/memories/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "user preferences",
    "namespace": "default",
    "limit": 10
  }'

# Get assembled context (position-aware)
curl -X POST http://localhost:8080/v1/context \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What should I know about this user?",
    "namespace": "default",
    "token_budget": 2000
  }'

================================================================================
MEMORY TYPES
================================================================================

MAIA supports three memory types, each optimized for different use cases:

SEMANTIC (default):
- Purpose: Facts, profiles, structured knowledge
- Example: "User is a senior developer at Acme Corp"
- Zone preference: Critical (high relevance, placed first)
- Retrieval: Primary focus on vector similarity

EPISODIC:
- Purpose: Conversations, experiences, temporal context
- Example: "Yesterday discussed migration strategy"
- Zone preference: Middle
- Retrieval: Balances relevance with recency

WORKING:
- Purpose: Current session state, transient information
- Example: "Currently editing file: main.go"
- Zone preference: Recency (placed last for recency effect)
- Retrieval: Always included if within time window

================================================================================
CONTEXT ASSEMBLY
================================================================================

POSITION-AWARE ZONES:

┌─────────────────────────────────────────┐
│  CRITICAL ZONE (15% of budget)          │  ← High-relevance facts (score ≥ 0.7)
│  Most important information first       │
├─────────────────────────────────────────┤
│  MIDDLE ZONE (65% of budget)            │  ← Supporting context
│  Decreasing relevance, detailed info    │
├─────────────────────────────────────────┤
│  RECENCY ZONE (20% of budget)           │  ← Recent/temporal context
│  Working memory, recent interactions    │
└─────────────────────────────────────────┘

This addresses:
- Primacy effect: Critical info first gets more attention
- Recency effect: Recent info last is remembered better
- Middle blindness: Supporting context in between

RETRIEVAL STRATEGY WEIGHTS (defaults):
- Vector Similarity: 35% (semantic matching via embeddings)
- Full-Text Search: 25% (keyword and phrase matching)
- Recency: 20% (favor recent information)
- Access Frequency: 10% (frequently accessed = important)
- Graph Connectivity: 10% (related memories boost score)

Score fusion uses Reciprocal Rank Fusion (RRF):
RRF_score = Σ (1 / (k + rank_i)) where k = 60

================================================================================
REST API REFERENCE
================================================================================

BASE URL: http://localhost:8080

AUTHENTICATION (if enabled):
- Header: X-API-Key: your-api-key
- Header: Authorization: Bearer your-api-key
- Query: ?api_key=your-api-key

HEALTH ENDPOINTS:

GET /health
Response: {"status":"ok"}

GET /ready
Response: {"status":"ready","checks":{"storage":"ok"}}

GET /metrics
Response: Prometheus format metrics

MEMORY ENDPOINTS:

POST /v1/memories
Create a memory.
Body: {
  "namespace": "default",        # Required
  "content": "string",           # Required (max 10KB)
  "type": "semantic",            # semantic|episodic|working
  "metadata": {},                # Optional key-value
  "tags": ["tag1", "tag2"],      # Optional
  "confidence": 0.9,             # 0.0-1.0
  "source": "user"               # user|extracted|inferred|imported
}

GET /v1/memories/:id
Get memory by ID.

PUT /v1/memories/:id
Update memory.
Body: { "content": "new content", "tags": ["new"], "confidence": 0.95 }

DELETE /v1/memories/:id
Delete memory.

POST /v1/memories/search
Search memories.
Body: {
  "query": "search text",
  "namespace": "default",
  "types": ["semantic"],
  "tags": ["tag"],
  "min_confidence": 0.5,
  "time_range": { "start": "ISO8601", "end": "ISO8601" },
  "limit": 20,
  "offset": 0
}

CONTEXT ENDPOINT:

POST /v1/context
Assemble position-aware context.
Body: {
  "query": "string",             # Required
  "namespace": "default",
  "token_budget": 4000,
  "system_prompt": "string",
  "include_scores": true,
  "min_score": 0.3,
  "memory_types": ["semantic", "episodic"],
  "zone_allocation": { "critical": 0.15, "middle": 0.65, "recency": 0.20 }
}
Response includes:
- content: Assembled context string
- memories: Array with id, content, type, score, position, token_count
- token_count: Actual tokens used
- zone_stats: Tokens used/budget per zone

NAMESPACE ENDPOINTS:

POST /v1/namespaces
Create namespace.
Body: {
  "name": "my-namespace",
  "parent": "parent-ns",         # Optional (hierarchical)
  "config": {
    "token_budget": 8000,
    "max_memories": 50000,
    "retention_days": 90
  }
}

GET /v1/namespaces
List namespaces.

GET /v1/namespaces/:id
Get namespace details.

PUT /v1/namespaces/:id
Update namespace config.

DELETE /v1/namespaces/:id
Delete namespace (use ?force=true for non-empty).

GET /v1/namespaces/:id/memories
List memories in namespace.

STATISTICS:

GET /v1/stats
Response: {
  "storage": { "memory_count": 15000, "namespace_count": 12, ... },
  "server": { "uptime_seconds": 86400, "requests_total": 150000, ... },
  "performance": { "avg_write_latency_ms": 1.2, ... }
}

OPENAI-COMPATIBLE PROXY:

POST /proxy/v1/chat/completions
Standard OpenAI chat completions format with automatic context injection.
Headers:
- X-MAIA-Namespace: target namespace (default: "default")
- X-MAIA-Skip-Memory: skip context injection (default: false)
- X-MAIA-Skip-Extract: skip memory extraction (default: false)
- X-MAIA-Token-Budget: context token budget (default: 4000)

================================================================================
MCP INTEGRATION
================================================================================

MAIA provides an MCP server for Claude Desktop and Cursor integration.

CLAUDE DESKTOP SETUP:

1. Build MCP server:
   make build-mcp
   # Binary: ./build/maia-mcp

2. Configure ~/Library/Application Support/Claude/claude_desktop_config.json:
   {
     "mcpServers": {
       "maia": {
         "command": "/path/to/maia-mcp",
         "env": {
           "MAIA_URL": "http://localhost:8080",
           "MAIA_DEFAULT_NAMESPACE": "claude"
         }
       }
     }
   }

3. Restart Claude Desktop

MCP TOOLS:

remember - Store information
  Parameters: content (required), namespace, type, tags, metadata
  Example: "Remember that I prefer TypeScript over JavaScript"

recall - Retrieve relevant context
  Parameters: query (required), namespace, token_budget, min_score

forget - Delete a memory
  Parameters: id (required)

list_memories - List memories
  Parameters: namespace, type, tags, limit, offset

get_context - Assemble position-aware context (advanced recall)
  Parameters: query (required), namespace, token_budget, system_prompt, include_scores

maia_complete - Generate completion with memory context (if inference enabled)
  Parameters: model (required), messages (required), namespace, token_budget,
              temperature, max_tokens, provider, inject_memory

maia_stream - Streaming completion with memory context (if inference enabled)
  Parameters: Same as maia_complete

maia_list_models - List available inference models
  Parameters: provider (optional filter)

MCP RESOURCES:
- maia://namespaces - List all namespaces
- maia://namespace/{namespace}/memories - List memories in namespace
- maia://memory/{id} - Get specific memory
- maia://stats - Server statistics

================================================================================
INFERENCE INTEGRATION
================================================================================

MAIA supports pluggable inference backends for memory-augmented generation.

SUPPORTED PROVIDERS:
- Ollama (local, no API key needed)
- OpenRouter (100+ models, cloud)
- Anthropic (Claude models)

CONFIGURATION:

inference:
  enabled: true
  default_provider: "ollama"

  providers:
    ollama:
      type: "ollama"
      base_url: "http://localhost:11434/v1"
      timeout: 60s
      models: ["llama2", "mistral", "codellama"]

    openrouter:
      type: "openrouter"
      base_url: "https://openrouter.ai/api/v1"
      api_key: "${OPENROUTER_API_KEY}"
      timeout: 120s

    anthropic:
      type: "anthropic"
      base_url: "https://api.anthropic.com/v1"
      api_key: "${ANTHROPIC_API_KEY}"
      timeout: 120s

  routing:
    model_mapping:
      "llama*": "ollama"
      "mistral*": "ollama"
      "claude*": "anthropic"
      "*": "openrouter"  # Default fallback

  cache:
    enabled: true
    ttl: 24h
    namespace: "inference:cache"

FEATURES:
- Automatic memory context injection into prompts
- Model-to-provider routing with wildcards
- Response caching with configurable TTL
- Health checking and automatic failover
- Streaming support with accumulation

================================================================================
CLI REFERENCE (maiactl)
================================================================================

CONFIGURATION:
export MAIA_URL=http://localhost:8080
export MAIA_API_KEY=your-key

MEMORY COMMANDS:

maiactl memory create -n default -c "content" -t semantic --tags "tag1,tag2"
maiactl memory list -n default
maiactl memory get <id>
maiactl memory update <id> -c "new content"
maiactl memory delete <id>
maiactl memory search -q "query" -n default --min-score 0.5

NAMESPACE COMMANDS:

maiactl namespace create my-project --token-budget 8000
maiactl namespace list
maiactl namespace get my-project
maiactl namespace update my-project --token-budget 10000
maiactl namespace delete my-project [-f]

CONTEXT COMMAND:

maiactl context "query" -n default -b 4000 [-z for zone stats]

OTHER COMMANDS:

maiactl stats              # Server statistics
maiactl version            # Version info
maiactl completion bash    # Shell completion

================================================================================
CONFIGURATION REFERENCE
================================================================================

CONFIGURATION SOURCES (precedence order):
1. Command-line flags (highest)
2. Environment variables (MAIA_ prefix)
3. Config file (config.yaml)
4. Default values (lowest)

KEY ENVIRONMENT VARIABLES:

Server:
MAIA_HTTP_PORT=8080
MAIA_GRPC_PORT=9090
MAIA_MAX_CONCURRENT_REQUESTS=100
MAIA_REQUEST_TIMEOUT=30s

Storage:
MAIA_DATA_DIR=./data
MAIA_SYNC_WRITES=false
MAIA_ENCRYPTION_KEY=           # 32-byte for at-rest encryption

Embedding:
MAIA_EMBEDDING_MODEL=local     # local|openai|voyage|mock
MAIA_EMBEDDING_DIMENSIONS=384
MAIA_OPENAI_API_KEY=
MAIA_VOYAGE_API_KEY=

Memory:
MAIA_DEFAULT_NAMESPACE=default
MAIA_DEFAULT_TOKEN_BUDGET=4000
MAIA_AUTO_CREATE_NAMESPACE=true

Retrieval:
MAIA_VECTOR_WEIGHT=0.35
MAIA_TEXT_WEIGHT=0.25
MAIA_RECENCY_WEIGHT=0.20
MAIA_FREQUENCY_WEIGHT=0.10
MAIA_GRAPH_WEIGHT=0.10

Context Assembly:
MAIA_ZONE_CRITICAL=0.15
MAIA_ZONE_MIDDLE=0.65
MAIA_ZONE_RECENCY=0.20
MAIA_CRITICAL_THRESHOLD=0.7
MAIA_RECENCY_WINDOW=24h

Proxy:
MAIA_PROXY_BACKEND=https://api.openai.com
MAIA_PROXY_BACKEND_API_KEY=
MAIA_PROXY_AUTO_REMEMBER=true
MAIA_PROXY_AUTO_CONTEXT=true
MAIA_PROXY_TOKEN_BUDGET=4000

Security:
MAIA_API_KEY=
MAIA_ENABLE_TLS=false
MAIA_TLS_CERT=
MAIA_TLS_KEY=
MAIA_RATE_LIMIT_RPS=100

Logging:
MAIA_LOG_LEVEL=info            # debug|info|warn|error
MAIA_LOG_FORMAT=json           # json|text

Multi-Tenancy:
MAIA_TENANCY_ENABLED=false
MAIA_TENANCY_DEFAULT_PLAN=free
MAIA_TENANCY_ENFORCE_QUOTAS=true

FULL CONFIG FILE EXAMPLE:

server:
  http_port: 8080
  max_concurrent_requests: 100
  request_timeout: 30s

storage:
  data_dir: ./data
  sync_writes: false

embedding:
  model: local
  dimensions: 384

memory:
  default_namespace: default
  default_token_budget: 4000

retrieval:
  vector_weight: 0.35
  text_weight: 0.25
  recency_weight: 0.20
  frequency_weight: 0.10
  graph_weight: 0.10

context:
  zone_critical: 0.15
  zone_middle: 0.65
  zone_recency: 0.20
  critical_threshold: 0.7
  recency_window: 24h

proxy:
  backend: https://api.openai.com
  backend_api_key: ${OPENAI_API_KEY}
  auto_remember: true
  auto_context: true

security:
  api_key: ${MAIA_API_KEY}
  rate_limit_rps: 100

logging:
  level: info
  format: json

================================================================================
MULTI-TENANCY
================================================================================

Enable multi-tenant mode for SaaS deployments:

tenancy:
  enabled: true
  default_plan: free
  enforce_quotas: true

PLANS AND DEFAULT QUOTAS:

Plan        Memories    Storage   RPM     RPD
Free        10,000      100 MB    100     10,000
Standard    100,000     1 GB      1,000   100,000
Premium     1,000,000   10 GB     10,000  1,000,000

ADMIN API:

POST /admin/tenants              - Create tenant
GET /admin/tenants               - List tenants
GET /admin/tenants/:id           - Get tenant
PUT /admin/tenants/:id           - Update tenant
DELETE /admin/tenants/:id        - Delete tenant
GET /admin/tenants/:id/usage     - Get usage stats
POST /admin/tenants/:id/suspend  - Suspend tenant
POST /admin/tenants/:id/activate - Activate tenant

TENANT ISOLATION:
- Namespaces prefixed: tenant-id/namespace-name
- API keys mapped to tenants via authorization config
- Quotas enforced per tenant

================================================================================
SDK USAGE
================================================================================

GO SDK:

import "github.com/ar4mirez/maia/pkg/maia"

client := maia.New(maia.WithBaseURL("http://localhost:8080"))

// Remember
mem, _ := client.Remember(ctx, "default", "User prefers dark mode")

// Recall
result, _ := client.Recall(ctx, "user preferences",
    maia.WithNamespace("default"),
    maia.WithTokenBudget(2000),
)

// Forget
client.Forget(ctx, mem.ID)

// Search
results, _ := client.Search(ctx, &maia.SearchMemoriesInput{
    Namespace: "default",
    Query:     "preferences",
})

TYPESCRIPT SDK:

import { MAIAClient } from '@maia/sdk';

const client = new MAIAClient({ baseUrl: 'http://localhost:8080' });

// Remember
const memory = await client.remember('default', 'User prefers dark mode');

// Recall
const context = await client.recall('user preferences', {
    namespace: 'default',
    tokenBudget: 2000,
});

// Forget
await client.forget(memory.id);

PYTHON SDK:

from maia import MAIAClient

client = MAIAClient(base_url="http://localhost:8080")

# Remember
memory = client.remember("default", "User prefers dark mode")

# Recall
context = client.recall("user preferences", namespace="default", token_budget=2000)

# Forget
client.forget(memory.id)

# Async client available
from maia import AsyncMAIAClient
async with AsyncMAIAClient() as client:
    memory = await client.remember("default", "content")

================================================================================
DEPLOYMENT
================================================================================

DOCKER:

docker run -d \
  --name maia \
  -p 8080:8080 \
  -v maia-data:/data \
  -e MAIA_API_KEY=your-key \
  ghcr.io/ar4mirez/maia:latest

DOCKER COMPOSE (with monitoring):

services:
  maia:
    image: ghcr.io/ar4mirez/maia:latest
    ports: ["8080:8080"]
    volumes: ["maia-data:/data"]
    environment:
      MAIA_LOG_LEVEL: info
      MAIA_TRACING_ENABLED: "true"
      MAIA_TRACING_ENDPOINT: jaeger:4318

  prometheus:
    image: prom/prometheus:latest
    ports: ["9090:9090"]

  grafana:
    image: grafana/grafana:latest
    ports: ["3000:3000"]

  jaeger:
    image: jaegertracing/all-in-one:latest
    ports: ["16686:16686", "4318:4318"]

KUBERNETES:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: maia
spec:
  replicas: 1
  template:
    spec:
      containers:
        - name: maia
          image: ghcr.io/ar4mirez/maia:latest
          ports:
            - containerPort: 8080
          env:
            - name: MAIA_DATA_DIR
              value: /data
            - name: MAIA_API_KEY
              valueFrom:
                secretKeyRef:
                  name: maia-secrets
                  key: api-key
          volumeMounts:
            - name: data
              mountPath: /data
          livenessProbe:
            httpGet: { path: /health, port: 8080 }
          readinessProbe:
            httpGet: { path: /ready, port: 8080 }
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: maia-data

SYSTEMD SERVICE:

[Unit]
Description=MAIA Memory Server
After=network.target

[Service]
Type=simple
User=maia
ExecStart=/usr/local/bin/maia --config /etc/maia/config.yaml
Restart=always

[Install]
WantedBy=multi-user.target

================================================================================
PERFORMANCE CHARACTERISTICS
================================================================================

LATENCY TARGETS (actual performance):
- Memory write: < 50ms p99 (actual ~1ms)
- Memory read: < 20ms p99 (actual ~0.5ms)
- Vector search (10k memories): < 50ms p99 (actual ~5ms)
- Context assembly (500 memories): < 200ms p99 (actual ~0.3ms)

THROUGHPUT:
- Concurrent reads: 80,000+ ops/sec
- Concurrent writes: 21,000+ ops/sec
- Mixed workload (80/20): 57,000+ ops/sec

MEMORY USAGE:
- Baseline: ~50MB (empty store)
- Per memory: ~1KB (compressed)
- Vector index: ~4KB per vector (384 dims)

================================================================================
ARCHITECTURE
================================================================================

COMPONENTS:

1. API Layer
   - REST API (Gin) - Universal HTTP interface
   - MCP Server (stdio) - Claude Desktop/Cursor integration
   - OpenAI Proxy - Drop-in replacement with context injection

2. Query Understanding
   - Intent detection (factual, procedural, conversational, exploratory, temporal)
   - Entity extraction (dates, keywords, names)
   - Context type detection
   - Temporal scoping

3. Retrieval Layer
   - Multi-strategy retrieval (vector, text, recency, frequency, graph)
   - Score fusion via RRF
   - Configurable weights

4. Index Layer
   - Vector Index: HNSW (M=16, EfConstruction=200, EfSearch=50)
   - Full-Text Index: Bleve v2 with stemming and tokenization
   - Graph Index: Adjacency list with relationship types

5. Embedding Layer
   - Local: ONNX runtime with all-MiniLM-L6-v2 (384 dims, ~10ms)
   - Remote: OpenAI, Voyage AI

6. Context Assembly
   - Position-aware zone assignment
   - Token budget management
   - Heuristic token estimation

7. Storage Layer
   - BadgerDB v4 (embedded LSM-tree)
   - Optional AES-256 encryption at rest

8. Inference Layer (optional)
   - Pluggable providers (Ollama, OpenRouter, Anthropic)
   - Model routing with wildcards
   - Response caching
   - Health checking and failover

KEY DESIGN DECISIONS:
- BadgerDB for single-binary deployment, no external dependencies
- Local embeddings first for latency (10ms vs 100ms) and privacy
- Position-aware assembly based on LLM attention research

================================================================================
ERROR HANDLING
================================================================================

ERROR RESPONSE FORMAT:
{
  "error": {
    "code": "not_found",
    "message": "Memory not found",
    "details": { "memory_id": "mem_invalid" }
  }
}

COMMON ERROR CODES:
- bad_request (400): Invalid request body or parameters
- unauthorized (401): Missing or invalid API key
- forbidden (403): Insufficient permissions
- not_found (404): Resource not found
- conflict (409): Resource already exists
- rate_limited (429): Rate limit exceeded
- internal_error (500): Server error

RATE LIMITING HEADERS:
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 0
X-RateLimit-Reset: 1706097600
Retry-After: 60

================================================================================
TROUBLESHOOTING
================================================================================

SERVER WON'T START:
- Check port availability: lsof -i :8080
- Enable debug logging: MAIA_LOG_LEVEL=debug maia
- Verify data directory permissions

HIGH MEMORY USAGE:
- Check stats: curl http://localhost:8080/v1/stats
- Trigger compaction: curl -X POST http://localhost:8080/admin/compact

SLOW QUERIES:
- Enable debug logging for query analysis
- Check retrieval weights configuration
- Review token budget (smaller = faster)

MCP NOT WORKING:
- Verify MAIA server running: curl http://localhost:8080/health
- Check Claude Desktop logs: ~/Library/Logs/Claude/
- Verify MCP server binary path in config

MEMORY NOT RETRIEVED:
- Verify namespace: maiactl namespace list
- Check memory exists: maiactl memory search -q "query" -n namespace
- Review min_score threshold

================================================================================
LINKS AND RESOURCES
================================================================================

GitHub: https://github.com/ar4mirez/maia
Issues: https://github.com/ar4mirez/maia/issues
Discussions: https://github.com/ar4mirez/maia/discussions

MCP Specification: https://modelcontextprotocol.io/

License: Apache 2.0
